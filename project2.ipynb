{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SN7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import rasterio\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "from rasterio.features import rasterize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per calcolare media e deviazione standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(image_paths):\n",
    "    means = []  \n",
    "    stds = []  \n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)  # Carica l'immagine inclusi tutti i canali\n",
    "        #img_rgb = img[:, :, :3]  # Prendo i primi tre canali (RGB)\n",
    "        #permute mi serve perchè img la leggo con opencv che mi da HxWxC e pytorch vuole CxHxW\n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # Converti in tensor e permuta le dimensioni a (Canali, Altezza, Larghezza)\n",
    "\n",
    "        means.append(img_tensor.mean(dim=(1, 2)))  # Calcola e aggiungi la media per ogni canale (RGB) alla lista delle medie (1=h, 2=w)\n",
    "        stds.append(img_tensor.std(dim=(1, 2)))    # Calcola e aggiungi la deviazione standard per ogni canale (RGB) alla lista delle deviazioni standard\n",
    "    \n",
    "    mean = torch.stack(means).mean(dim=0)  # Combina tutte le medie delle immagini e calcola la media finale per ogni canale\n",
    "    std = torch.stack(stds).mean(dim=0)    # Combina tutte le deviazioni standard delle immagini e calcola la deviazione standard finale per ogni canale\n",
    "    \n",
    "    return mean.tolist(), std.tolist()  # Converte i tensori risultanti in liste e li restituisce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media dei canali RGBM: [82.15846252441406, 110.30465698242188, 126.20237731933594, 254.79959106445312]\n",
      "Deviazione standard dei canali RGBM: [27.31522560119629, 30.465126037597656, 38.47288513183594, 3.201913833618164]\n"
     ]
    }
   ],
   "source": [
    "# Path alle immagini nella cartella \"training/images\"\n",
    "image_paths = glob('training/images/*.tif')  \n",
    "\n",
    "# Esegui la computazione su GPU se disponibile\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "mean, std = compute_mean_std(image_paths)\n",
    "\n",
    "print(f'Media dei canali RGBM: {mean}')\n",
    "print(f'Deviazione standard dei canali RGBM: {std}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborazione label:\n",
    "La gestione dell'eccezione: ValueError è dovuta alle immagini e label native del dataset, in pratica in alcune immagini satellitari, c'è la presenza di nuvole, oppure rumore di qualche altro tipo. Per gestire questo rumore è stato creato un 4 canale, che effettua una maschera coprente  sulle zone dove c'è rumore/nuvole. Ci sono anche alcune labels apposite per le maschere. In casi eccezzionali l'intera immagine ha la maschera, dove avviene il mascheramento, le labels ordinarie sono prive di dati geojson, quindi se l'intera immagine è mascherata alcune label sono vuote e quindi cioò comporta quell'errore, che vista l'assenza di label, ho deciso di gestire la problematica semplicemente creando una maschera nera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisce una funzione per creare una maschera binaria da geometrie\n",
    "def create_mask(geojson, img_shape, transform):\n",
    "    shapes = [(geom, 1) for geom in geojson.geometry if geom.is_valid and not geom.is_empty]\n",
    "    if not shapes:\n",
    "        raise ValueError('No valid geometry objects found for rasterize')\n",
    "    mask = rasterize(shapes=shapes, out_shape=img_shape, transform=transform, fill=0, dtype=np.uint8)\n",
    "    return mask\n",
    "\n",
    "# Salva la maschera binaria normalizzata per visualizzazione\n",
    "def save_normalized_mask(mask, output_path, crs, transform):\n",
    "    mask_normalized = (mask * 255).astype(np.uint8)\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=mask_normalized.shape[0],\n",
    "        width=mask_normalized.shape[1],\n",
    "        count=1,\n",
    "        dtype=mask_normalized.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(mask_normalized, 1)\n",
    "\n",
    "def process_dataset(image_dir, label_dir, output_dir):\n",
    "    image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "    label_paths = sorted(glob(os.path.join(label_dir, '*.geojson')))\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    for img_path, label_path in zip(image_paths, label_paths):\n",
    "        labels_match = gpd.read_file(label_path)\n",
    "\n",
    "        with rasterio.open(img_path) as src:\n",
    "            img_shape = (src.height, src.width)\n",
    "            transform = src.transform\n",
    "            img_crs = src.crs\n",
    "\n",
    "        try:\n",
    "            mask_labels_match = create_mask(labels_match, img_shape, transform)\n",
    "        except ValueError as e:\n",
    "            print(f\"Empty or invalid label, noise on image: {img_path} due to error: {e}\")\n",
    "            mask_labels_match = np.zeros(img_shape, dtype=np.uint8)  # Crea una maschera nera in caso di errore\n",
    "        \n",
    "        img_name = os.path.basename(img_path)\n",
    "        mask_name = img_name.replace('.tif', '_label_mask.tif')\n",
    "        mask_output_path = os.path.join(output_dir, mask_name)\n",
    "\n",
    "        save_normalized_mask(mask_labels_match, mask_output_path, img_crs, transform)\n",
    "        \n",
    "        processed_count += 1\n",
    "\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f'Processed: {processed_count} masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = 'training/images_masked'\n",
    "label_dir_train = 'training/labels/labels_match'\n",
    "mask_dir_train = 'training/mask_from_label'\n",
    "\n",
    "image_dir_valid = 'validation/images_masked'\n",
    "label_dir_valid = 'validation/labels/labels_match'\n",
    "mask_dir_valid = 'validation/mask_from_label'\n",
    "\n",
    "image_dir_test = 'test/images_masked'\n",
    "label_dir_test = 'test/labels/labels_match'\n",
    "mask_dir_test = 'test/mask_from_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_dataset(image_dir_train, label_dir_train, mask_dir_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_dataset(image_dir_valid, label_dir_valid, mask_dir_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_dataset(image_dir_test, label_dir_test, mask_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training/train.txt, validation/val.txt, and test/test.txt with image and mask references.\n"
     ]
    }
   ],
   "source": [
    "# Output file paths\n",
    "train_txt_path = os.path.join('training', 'train.txt')\n",
    "val_txt_path = os.path.join('validation', 'val.txt')\n",
    "test_txt_path = os.path.join('test', 'test.txt')\n",
    "\n",
    "# Function to create txt files with image and mask paths\n",
    "def create_txt_file(image_dir, mask_dir, output_txt_path):\n",
    "    image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "    mask_paths = sorted(glob(os.path.join(mask_dir, '*.tif')))\n",
    "\n",
    "    if len(image_paths) != len(mask_paths):\n",
    "        raise ValueError(f\"The number of images and masks do not match in {image_dir} and {mask_dir}\")\n",
    "\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "            f.write(f\"{img_path} {mask_path}\\n\")\n",
    "\n",
    "# Create train.txt, val.txt, and test.txt\n",
    "create_txt_file(image_dir_train, mask_dir_train, train_txt_path)\n",
    "create_txt_file(image_dir_valid, mask_dir_valid, val_txt_path)\n",
    "create_txt_file(image_dir_test, mask_dir_test, test_txt_path)\n",
    "\n",
    "print(f\"Created {train_txt_path}, {val_txt_path}, and {test_txt_path} with image and mask references.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_txt_path, 'r') as f:\n",
    "    train_data = [line.split() for line in f.read().splitlines()]\n",
    "\n",
    "train_image_paths = [line[0] for line in train_data]\n",
    "train_label_paths = [line[1] for line in train_data]\n",
    "\n",
    "with open(val_txt_path, 'r') as f:\n",
    "    val_data = [line.split() for line in f.read().splitlines()]\n",
    "\n",
    "val_image_paths = [line[0] for line in val_data]\n",
    "val_label_paths = [line[1] for line in val_data]\n",
    "\n",
    "with open(test_txt_path, 'r') as f:\n",
    "    test_data = [line.split() for line in f.read().splitlines()]\n",
    "\n",
    "test_image_paths = [line[0] for line in test_data]\n",
    "test_label_paths = [line[1] for line in test_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read().transpose((1, 2, 0))\n",
    "            image = image.astype(np.float32)\n",
    "            image = (image - mean) / std\n",
    "\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "            mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DoubleConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "      super(DoubleConv, self).__init__()\n",
    "      self.conv = nn.Sequential(\n",
    "          nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "          nn.ReLU(inplace=True),\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "  def __init__(\n",
    "      self, in_channels=4, out_channels=1, features=[64, 128, 256, 512],\n",
    "  ):\n",
    "    super(UNET, self).__init__()\n",
    "    self.ups = nn.ModuleList()\n",
    "    self.downs = nn.ModuleList()\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    for feature in features:\n",
    "      self.downs.append(DoubleConv(in_channels, feature))\n",
    "      in_channels = feature\n",
    "\n",
    "    for feature in reversed(features):\n",
    "      self.ups.append(\n",
    "          nn.ConvTranspose2d(\n",
    "              feature*2, feature, kernel_size=2, stride=2,\n",
    "          )\n",
    "      )\n",
    "      self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "  def forward(self, x):\n",
    "    skip_connections = []\n",
    "\n",
    "    for down in self.downs:\n",
    "      x = down(x)\n",
    "      skip_connections.append(x)\n",
    "      x = self.pool(x)\n",
    "\n",
    "    x = self.bottleneck(x)\n",
    "    skip_connections = skip_connections[::-1]\n",
    "\n",
    "    for idx in range(0, len(self.ups), 2):\n",
    "      x = self.ups[idx](x)\n",
    "      skip_connection = skip_connections[idx//2]\n",
    "\n",
    "      if x.shape != skip_connection.shape:\n",
    "        x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "      concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "      x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "    return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler, txtfile):\n",
    "    loop = tqdm(loader)\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.float().to(device=device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data).squeeze(1)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        tot_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = tot_loss / len(loader)\n",
    "    with open(txtfile, \"a\") as f:\n",
    "        f.write(f\"Train Loss: {avg_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni di mantenimento addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"/NN_register/checkpoint.pth.tar\"):\n",
    "  print(\"=> Saving checkpoint\")\n",
    "  torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "  print(\"=> Loading checkpoint\")\n",
    "  model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(loader, model, loss_fn, txtfile, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    total_dice_score = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_it, (x, y) in enumerate(loader, start=1):\n",
    "            print(num_it)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x).squeeze(1)\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(out)\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "            num_correct += (preds == y).sum().item()\n",
    "            num_pixels += torch.numel(preds)\n",
    "\n",
    "            dice_score = (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "            total_dice_score += dice_score.item()\n",
    "\n",
    "            y_np = y.cpu().numpy().flatten()\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "            total_precision += precision_score(y_np, preds_np, zero_division=1)\n",
    "            total_recall += recall_score(y_np, preds_np, zero_division=1)\n",
    "            total_f1 += f1_score(y_np, preds_np, zero_division=1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_dice_score = total_dice_score / len(loader)\n",
    "    avg_precision = total_precision / len(loader)\n",
    "    avg_recall = total_recall / len(loader)\n",
    "    avg_f1 = total_f1 / len(loader)\n",
    "    accuracy = num_correct / num_pixels * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Dice score: {avg_dice_score:.4f}\")\n",
    "    print(f\"Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Recall: {avg_recall:.4f}\")\n",
    "    print(f\"F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "    with open(txtfile, \"a\") as f:\n",
    "        f.write(f\"Validation Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Dice score: {avg_dice_score:.4f}\\n\")\n",
    "        f.write(f\"Precision: {avg_precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {avg_f1:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    model.train()\n",
    "    return avg_loss, accuracy, avg_dice_score, avg_precision, avg_recall, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(loader, model, folder, device=\"cuda\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    model.eval()\n",
    "    it = 1\n",
    "    for idx, (x, y) in enumerate(tqdm(loader, desc=\"Saving predictions\")):\n",
    "        print(it)\n",
    "        it += 1\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "        combined = torch.cat((y, preds), dim=1)  # Concatenate along height (dim=1)\n",
    "        torchvision.utils.save_image(combined, f\"{folder}/comparison_{idx}.tif\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file not found, starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(height=360, width=360),\n",
    "    # A.CenterCrop(height=896, width=896),\n",
    "    # A.RandomCrop(height=320, width=320),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.VerticalFlip(p=0.5),\n",
    "    # A.RandomRotate90(p=1),\n",
    "    # A.Rotate(limit=90, p=0.5),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=360, width=360),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_dataset = SARDataset(train_image_paths, train_label_paths, transform=transform)\n",
    "val_dataset = SARDataset(val_image_paths, val_label_paths, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "load_model = True\n",
    "lr = 1e-5\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "model_dir = os.path.join(base_dir, \"NN_register\")\n",
    "output_dir = os.path.join(model_dir, \"output\")\n",
    "image_output_dir = os.path.join(model_dir, \"images\")\n",
    "\n",
    "# Assicurati che le directory di output esistano\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "step_size = 18\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNET(in_channels=4, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=step_size)\n",
    "num_epochs = 20\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir, \"checkpoint.pth.tar\")\n",
    "if load_model and os.path.exists(checkpoint_path):\n",
    "    load_checkpoint(torch.load(checkpoint_path), model)\n",
    "else:\n",
    "    print(\"Checkpoint file not found, starting training from scratch.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.27it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.4566\n",
      "Accuracy: 91.79\n",
      "Dice score: 0.0099\n",
      "Precision: 0.0699\n",
      "Recall: 0.0054\n",
      "F1 Score: 0.0099\n",
      "=> Saving checkpoint\n",
      "New best F1 score: 0.0099\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.22it/s, loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.4265\n",
      "Accuracy: 92.16\n",
      "Dice score: 0.0029\n",
      "Precision: 0.0727\n",
      "Recall: 0.0015\n",
      "F1 Score: 0.0029\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.24it/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.4076\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0003\n",
      "Precision: 0.0954\n",
      "Recall: 0.0001\n",
      "F1 Score: 0.0003\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.18it/s, loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3952\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0001\n",
      "Precision: 0.1034\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0001\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.22it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3856\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0001\n",
      "Precision: 0.1760\n",
      "Recall: 0.0001\n",
      "F1 Score: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  15%|█▌        | 2/13 [00:00<00:04,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  31%|███       | 4/13 [00:01<00:02,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  46%|████▌     | 6/13 [00:01<00:01,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  62%|██████▏   | 8/13 [00:02<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  77%|███████▋  | 10/13 [00:02<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  92%|█████████▏| 12/13 [00:02<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions: 100%|██████████| 13/13 [00:02<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.23it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3782\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0001\n",
      "Precision: 0.1473\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0001\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.22it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3700\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.20it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3562\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 0.0673\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.20it/s, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3495\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 0.0673\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.18it/s, loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3425\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:   8%|▊         | 1/13 [00:00<00:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  23%|██▎       | 3/13 [00:01<00:03,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  38%|███▊      | 5/13 [00:01<00:01,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  54%|█████▍    | 7/13 [00:01<00:01,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  69%|██████▉   | 9/13 [00:02<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions:  85%|████████▍ | 11/13 [00:02<00:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving predictions: 100%|██████████| 13/13 [00:02<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.19it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3367\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.22it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Validation Loss: 0.3350\n",
      "Accuracy: 92.29\n",
      "Dice score: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.20it/s, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m save_checkpoint(checkpoint, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m val_loss, val_accuracy, val_dice_score, val_precision, val_recall, val_f1 \u001b[38;5;241m=\u001b[39m \u001b[43meval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_f1 \u001b[38;5;241m>\u001b[39m best_f1_score:\n\u001b[1;32m     16\u001b[0m     best_f1_score \u001b[38;5;241m=\u001b[39m val_f1\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36meval_fn\u001b[0;34m(loader, model, loss_fn, txtfile, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m         total_precision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision_score(y_np, preds_np, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m         total_recall \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recall_score(y_np, preds_np, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m         total_f1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m     35\u001b[0m avg_dice_score \u001b[38;5;241m=\u001b[39m total_dice_score \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1279\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1100\u001b[0m     {\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1127\u001b[0m ):\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \n\u001b[1;32m   1130\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1292\u001b[0m     {\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1321\u001b[0m ):\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;124;03m    0.12...\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1779\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1779\u001b[0m MCM \u001b[38;5;241m=\u001b[39m \u001b[43mmultilabel_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamplewise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplewise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m tp_sum \u001b[38;5;241m=\u001b[39m MCM[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1787\u001b[0m pred_sum \u001b[38;5;241m=\u001b[39m tp_sum \u001b[38;5;241m+\u001b[39m MCM[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:517\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    408\u001b[0m     {\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, samplewise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    419\u001b[0m ):\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute a confusion matrix for each class or sample.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.21\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m            [1, 2]]])\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m column_or_1d(sample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:100\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 100\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py:399\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(y\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    400\u001b[0m         _assert_all_finite(data, input_name\u001b[38;5;241m=\u001b[39minput_name)\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2317\u001b[0m, in \u001b[0;36m_any_dispatcher\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m   2313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2314\u001b[0m                           initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m-> 2317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2318\u001b[0m                     where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_any_dispatcher)\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21many\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_f1_score = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    train_fn(train_loader, model, optimizer, criterion, scaler, os.path.join(output_dir, \"output.txt\"))\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint, os.path.join(model_dir, \"checkpoint.pth.tar\"))\n",
    "\n",
    "    val_loss, val_accuracy, val_dice_score, val_precision, val_recall, val_f1 = eval_fn(val_loader, model, criterion, os.path.join(output_dir, \"output.txt\"), device=device)\n",
    "\n",
    "    if val_f1 > best_f1_score:\n",
    "        best_f1_score = val_f1\n",
    "        save_checkpoint(checkpoint, filename=os.path.join(model_dir, \"best_model.pth.tar\"))\n",
    "        print(f\"New best F1 score: {best_f1_score:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_predictions_as_imgs(val_loader, model, image_output_dir, device=device)\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
